import { marked } from 'marked';

const markDownTemp = marked(
    '# 1. 聚类学习\n' +
        '\n' +
        '## 1.1. 相关概念\n' +
        '\n' +
        '- 聚类：数据对象的集合\n' +
        '\t- 同一个类中，数据对象是相似的\n' +
        '\t- 反之，不同类不相似\n' +
        '- 聚类算法\n' +
        '\t- 根据给定的标准，加数据集划分成几个聚类\n' +
        '\n' +
        '- 划分标准\n' +
        '\t- 特征向量看成特征空间的点，分类依据为点之间的距离（相似度）\n' +
        '\n' +
        '- 类特征\n' +
        '\t- 聚类学习中的类不是是先给定，而是根据数据的相似度和距离划分（<font color="#ff0000">无监督学习</font>）\n' +
        '\t- 聚类的数目和结构没有事先给定\n' +
        '\n' +
        '- 聚类的目的\n' +
        '\t- 潜在的自然分组结构\n' +
        '\t- 感兴趣的关系\n' +
        '\n' +
        '- 聚类的关键\n' +
        '\t- 特征的设计和选择\n' +
        '\t- 距离函数\n' +
        '\n' +
        '- 有效性\n' +
        '\t- 聚类分析方法是否有效与<font color="#ff0000">数据分布</font>有很大关系\n' +
        '\t- <font color="#ff0000">特征选择</font>很重要，外在表现为数据分布是否可分\n' +
        '\n' +
        '# 2. 距离度量\n' +
        '\n' +
        '目的：度量同类样本间的<font color="#ff0000">相似度</font>和不同样本间的<font color="#ff0000">差异性</font>\n' +
        '\n' +
        '## 2.1. 度量函数和度量空间\n' +
        '\n' +
        '数学定义；->并不是所有函数都是度量函数\n' +
        '\n' +
        '1. 非负性\n' +
        '2. 唯一性\n' +
        '3. 对称性\n' +
        '4. 三角不等式\n' +
        '\n' +
        '[[2023Fall/机器学习/0-introduction#距离度量函数|度量函数]]\n' +
        '\n' +
        '欧氏距离：两个向量的二范数  \n' +
        '曼哈顿距离：一范数  \n' +
        '切比雪夫：无穷范数\n' +
        '\n' +
        'MInjowski distance\n' +
        '\n' +
        '# 3. 聚类准则\n' +
        '\n' +
        '定义例子：设集合S中任意元素$x_i$和$x_j$间的距离有 $d(x_i,x_{j)\\le}h$ ，其中h为给定的阈值，称S对于阈值h组成一类\n' +
        '\n' +
        '## 3.1. 试探方法\n' +
        '\n' +
        '凭直觉或经验，对实际问题定义一种距离度量的<font color="#ff0000">阈值</font>，然后按<font color="#ff0000">最邻近规则</font>指定某些样本属于某个聚类类别\n' +
        '\n' +
        '## 3.2. 聚类准则函数方法\n' +
        '\n' +
        '一种定义：\n' +
        '\n' +
        '$J=\\sum\\limits_{j=1}^{c}\\sum\\limits_{x\\in S_j}|x-m_j|^2$\n' +
        '\n' +
        '# 4. 聚类方法\n' +
        '\n' +
        '- 基于试探的聚类搜索算法\n' +
        '- 系统聚类法\n' +
        '- 动态聚类法\n' +
        '\n' +
        '## 4.1. 基于试探的聚类搜索算法\n' +
        '\n' +
        '按最近邻规则的简单试探法\n' +
        '\n' +
        '基于最大最小距离的试探法\n' +
        '\n' +
        '## 4.2. 系统聚类法\n' +
        '\n' +
        '将数据样本<font color="#ff0000">按照距离准则逐步分类</font>，类别分别由多到少，直到获得合适的分类要求为止。\n' +
        '\n' +
        '### 4.2.1. 算法流程\n' +
        '\n' +
        '![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F09%2F19%2Fd69fb928b621e491fd856969bef3b2d1_20230919184351.png)\n' +
        '\n' +
        '### 4.2.2. 距离准则函数\n' +
        '\n' +
        '计算**类与类**之间的距离。\n' +
        '\n' +
        '- 主要计算准则\n' +
        '\t- 最短距离 （两个集合所有距离最小值）\n' +
        '\t- 最长距离（两个集合所有距离最大值）\n' +
        '\t- 类平均距离（所有距离平均值）\n' +
        '\n' +
        '## 4.3. 动态聚类法\n' +
        '\n' +
        '### 4.3.1. 基本思想\n' +
        '\n' +
        '1. 首先选择若干个样本点作为聚类中心，再按照某种准则使得样本点向各中心聚集，从而获得初始聚类；\n' +
        '2. 然后判断初始分类是否合理，不合理则修改；\n' +
        '3. 如此反复修改聚类\n' +
        '\n' +
        '### 4.3.2. K-means算法\n' +
        '\n' +
        '算法过程：\n' +
        '\n' +
        '1. 选择一个聚类数量k\n' +
        '2. 初始化聚类中心$u_1,\\dots,u_k$\n' +
        '   - 随机选择k个样本点，设置这些样本点为中心\n' +
        '3. 对每个样本点，计算样本点到k个聚类中心的距离（某种距离度量方法），将样本点**分配到距离它最近的聚类中心的聚类**\n' +
        '4. **重新计算聚类中心**，聚类中心为属于这一聚类的所有样本的均值\n' +
        '5. 如果没有发生**样本所属的聚类发生变化**的情况，则退出；否则，返回step3继续。\n' +
        '\n' +
        '- 影响因素\n' +
        '\t- k值\n' +
        '\t- 初始聚类中心\n' +
        '\t- 样本的几何分布\n' +
        '\n' +
        '实际应用中，需要试探不同的k值和选择不同的聚类中心的起始值。  \n' +
        '每次运行结果可能完全不一样，可以固定种子把随机数固定下来。\n' +
        '\n' +
        'k-means算法比较适用于分类数目已知的情况\n' +
        '\n' +
        '### 4.3.3. k-means++算法\n' +
        '\n' +
        '基本思想：K个初始聚类中心相互分得越开越好（**不再随机选择初始的k个聚类中心**）\n' +
        '\n' +
        '算法过程：\n' +
        '\n' +
        '1. 从数据集中随机选取一个样本作为初始聚类中心$c_1$\n' +
        '2. 首先计算每个样本与当前已有的聚类中心之间的最短距离（即与最近的一个聚类中心的距离）$D(x)$；接着计算每个样本被选为下一个聚类中心的概率为$\\frac{D(x)^2}{\\sum\\limits_{s\\in X}D(x)^2}$。最后，按照轮盘发选择出**下一个聚类中心**\n' +
        '3. 重复第2步知道选择出$K$个聚类中心\n' +
        '4. 之后步骤与K-means算法中的2-4步相同\n' +
        '\n' +
        '### 4.3.4. ISODATA算法\n' +
        '\n' +
        '有分裂/合并操作\n' +
        '\n' +
        '**算法过程**：\n' +
        '\n' +
        '1. 从数据集随机选取$K_0$个样本作为初始聚类中心$C=\\{c_1,c_2,...,c_{K_0}\\}$\n' +
        '2. 对每个样本$x_i$，计算它到$K_0$个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中\n' +
        '3. 判断上述每个类中的元素数目是否少于$N_{min}$。如果小于，则需要丢弃该类，令$K=K-1$，并把该类的样本重新分配到剩下类中距离最小的类\n' +
        '4. 针对每个$c_i$重新计算聚类中心\n' +
        '5. 如果$K\\le \\frac{K_0}{2}$，说明当前类别数太少，分裂\n' +
        '6. 如果$K\\ge \\frac{K_0}{2}$，说明当前类别数太少，合并\n' +
        '7. 如果达到最大迭代次数则终止，否则回到2\n' +
        '\n' +
        '**合并**：\n' +
        '\n' +
        '1. 计算当前所有类别的聚类中心两两之间的距离，用矩阵D表示，其中$D(i,i)=0$\n' +
        '2. 对于$D(i,j)\\lt d_{min}(i \\ne j)$的两个类别需要进行的合并操作，变成一个新的类，该类的聚类中心位置为:\n' +
        '\t- $m_{new}=\\frac{1}{n_i+n_j}(n_{i}m_{i}+n_{j}m_{j})$\n' +
        '\t- 两个类别加权求和\n' +
        '\n' +
        '两个类别允许最小距$d_{min}$：是否进行合并的阈值\n' +
        '\n' +
        '**分类**：\n' +
        '\n' +
        '1. 计算每个类别下所有样本在每个维度下的方差\n' +
        '2. 针对每个类别的所有方差挑出最大的方差$\\delta_{max}$  \n' +
        '3. 如果某个类别$\\delta_{max}\\lt Sigma$并且岩本数量$n_{i}\\le 2n_{min}$则可以分裂\n' +
        '4. 满足第3步条件的类分成两个子类并令$K=K+1$\n' +
        '\t- 新聚类的中心位置：$m_i^{(+)}=m_i+\\delta_{max},m_i^{(-)}=m_i-\\delta_{max}$  \n' +
        '最大方差Sigma：衡量样本分散程度\n' +
        '\n' +
        '## 4.4. 比较\n' +
        '\n' +
        '1. K-means通常适用于类别数目已知的聚类，ISODATA更灵活\n' +
        '2. ISODATA算法与K-means算法相似，聚类中心都是样本均值的迭代运算决定的\n' +
        '3. 都需要合理选择超参数\n' +
        '\n' +
        '# 5. 聚类评价\n' +
        '\n' +
        '考虑从一下几个指标评价聚类效果：\n' +
        '1. 聚类中心之间的距离\n' +
        '2. 聚类域的样本数目\n' +
        '3. 聚类域内的样本距离方差\n' +
        '\n' +
        '紧密度：$\\overline{CP_i}$每个簇内部的紧密程度\n' +
        '\n' +
        '间隔度：$\\overline{SP}$簇间是否分散\n' +
        '\n' +
        'DBI：max含义->看最差的情况\n' +
        '\n' +
        '\n' +
        'DVI\n' +
        '\n' +
        '\n' +
        '- 标签已知时的评价标准：\n' +
        '\t- P61\n' +
        '\n' +
        '\n' +
        '\n' +
        '# 6. 自监督学习\n' +
        '\n' +
        '- 无监督学习的一种形式，没有人类标注的监督信息\n' +
        '- 需要定义一个<font color="#ff0000">前置（借口）任务</font>，让网络学习我们关心的事情\n' +
        '- 对于大部分前置任务，需要保留一部分数据，让网络学会预测\n' +
        '- 通过前置任务学习到的特征会被用到不同的下游任务（通常包含标注）\n' +
        '\n' +
        '- 分类\n' +
        '\t- 前置任务学习\n' +
        '\t\t- 生成式方法：图像着色、修复\n' +
        '\t- 对比学习\n' +
        '\t- 非对比学习\n' +
        '\n' +
        '\n' +
        '## 6.1. 前置任务学习\n' +
        '\n' +
        '### 6.1.1. 生成式方法-图像着色\n' +
        '\n' +
        '![image.png](https://chillcharlie-img.oss-cn-hangzhou.aliyuncs.com/image%2F2023%2F09%2F19%2Fb88a34e9296c5ce5244cb2d9a0684b25_20230919205843.png)\n' +
        '\n' +
        '$x \\rightarrow CNN \\rightarrow \\overline{x}^`$\n' +
        '输入$(x,\\overline{x})$ 即(灰度化后的图像，原本的彩色图像)\n' +
        '$lose=||\\overline{x}^`-\\overline{x}||$，对比生产的彩色图像和原本彩色图像\n' +
        '\n' +
        '**数据里没有监督信息，构造假的监督信号**\n' +
        '\n' +
        '\n' +
        '\n' +
        '\n',
);

export default markDownTemp;
